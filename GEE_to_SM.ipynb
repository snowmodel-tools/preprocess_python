{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio as rio \n",
    "from datetime import datetime, timedelta\n",
    "from affine import Affine\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give location of folder containing files from GEE\n",
    "GEEpath='/nfs/attic/dfh/Aragon2/CSOdmn/WY/GEE_WY/';\n",
    "\n",
    "#Info re: the files names out of Crumley GEE script\n",
    "#give the 'root' pathname of the met files. Note: we will append things like _elev.tif,\n",
    "# _prec.tif, and so on...\n",
    "\n",
    "metfilename='cfsv2_2018-09-01_2019-10-01';\n",
    "\n",
    "#give names of dem and land cover\n",
    "demname='DEM_WY.tif';\n",
    "lcname='NLCD2016_WY.tif';\n",
    "\n",
    "#give name of domain (e.g., GOA, or Thompson_Pass, or something like that).\n",
    "#This will only be used for option lat / lon grids.\n",
    "domain='WY';\n",
    "\n",
    "#give the desired output name of the met file\n",
    "outfilename='mm_WY_2018-2020.dat'; #please use something descriptive to help identify the output file.\n",
    "\n",
    "#give start time information\n",
    "startyear=2018;\n",
    "startmonth=9;\n",
    "startday=1;\n",
    "pointsperday=4; #use 4 for 6-hourly data, 8 for 3-hourly data, etc.\n",
    "starthour=0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landcover \n",
    "\n",
    "### NLCD LC Codes\n",
    "\n",
    "|Code|NLCD2016 Landclass|Code|NLCD2016 Landclass|\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|11 | open water |51 | dwarf shrub|\n",
    "|12| ice / snow |52 | shrub/scrub|\n",
    "|21 | developed; open space |71 | grassland/herbaceous|\n",
    "|22 | developed; low intensity|72 | hedge/herbaceous|\n",
    "|23 | developed; med intensity|73 | lichens|\n",
    "|24 | developed; high intensity|74 | moss|\n",
    "|31 | barren; rock, sand, clay|81 | pasture/hay|\n",
    "|41 | deciduous forest|82 | cultivated crops|\n",
    "|42 | evergreen forest|90 | woody wetlands|\n",
    "|43 | mixed shrub|95 | emergent herbaceous wetlands|\n",
    "\n",
    "\n",
    "### Snowmodel LC Codes\n",
    "\n",
    "|Code  |Landcover Class |Code  |Landcover Class |\n",
    "| --- | --- | --- | --- |\n",
    "|1     | coniferous forest |13    | subalpine meadow  |      \n",
    "|2     | deciduous forest |14    | tundra (non-tussock) |      \n",
    "|3     | mixed forest |15    | tundra (tussock) |           \n",
    "|4     | scattered short-conifer |16    | prostrate shrub tundra | \n",
    "|5     | clearcut conifer |17    | arctic gram. wetland |       \n",
    "|6     | mesic upland shrub |18    | bare |       \n",
    "|7     | xeric upland shrub |19    | water/possibly frozen |       \n",
    "|8     | playa shrubland |20    | permanent snow/glacier |         \n",
    "|9     | shrub wetland/riparian |21    | residential/urban |   \n",
    "|10    | erect shrub tundra |22    | tall crops |       \n",
    "|11    | low shrub tundra |23    | short crops |        \n",
    "|12    | grassland rangeland  |24    | ocean |    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landcover data \n",
    "INpath = GEEpath+lcname\n",
    "def LC2SM(INpath,OUTpath):\n",
    "    src=rio.open(INpath)\n",
    "    lc=src.read(1)\n",
    "    #ascii header \n",
    "    head = \"ncols \"+str(np.shape(dem)[1])+\"\\n\\\n",
    "    nrows \"+str(np.shape(dem)[0])+\"\\n\\\n",
    "    xllcorner     \"+str(int(src.bounds.left))+\"\\n\\\n",
    "    yllcorner     \"+str(int(src.bounds.bottom))+\"\\n\\\n",
    "    cellsize      \"+str(int(src.res[0]))+\"\\n\\\n",
    "    NODATA_value  -9999\\n\"\n",
    "\n",
    "    #reassign lc from NLCD to SM classes\n",
    "    DIR=DIR=np.empty([np.shape(lc)[0],np.shape(lc)[1]])\n",
    "    DIR[lc == 11 ]=24\n",
    "    DIR[lc == 12 ]=20\n",
    "    DIR[lc == 21 ]=21\n",
    "    DIR[lc == 22 ]=21\n",
    "    DIR[lc == 23 ]=21\n",
    "    DIR[lc == 24 ]=21\n",
    "    DIR[lc == 31 ]=18\n",
    "    DIR[lc == 41 ]=2\n",
    "    DIR[lc == 42 ]=1\n",
    "    DIR[lc == 43 ]=6\n",
    "    DIR[lc == 51 ]=6\n",
    "    DIR[lc == 52 ]=6\n",
    "    DIR[lc == 71 ]=12\n",
    "    DIR[lc == 72 ]=12\n",
    "    DIR[lc == 73 ]=12\n",
    "    DIR[lc == 74 ]=12\n",
    "    DIR[lc == 81 ]=23\n",
    "    DIR[lc == 82 ]=22\n",
    "    DIR[lc == 90 ]=9\n",
    "    DIR[lc == 95 ]=9\n",
    "    DIR.astype(int)\n",
    "    np.savetxt(OUTpath+'NLCD2016_'+domain+'.asc', DIR, fmt='%4.0f', header = head,comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem\n",
    "INpath = GEEpath+demname\n",
    "def DEM2SM(INpath, OUTpath):\n",
    "    src=rio.open(INpath)\n",
    "    dem=src.read(1)\n",
    "    #ascii header \n",
    "    head = \"ncols \"+str(np.shape(dem)[1])+\"\\n\\\n",
    "    nrows \"+str(np.shape(dem)[0])+\"\\n\\\n",
    "    xllcorner     \"+str(int(src.bounds.left))+\"\\n\\\n",
    "    yllcorner     \"+str(int(src.bounds.bottom))+\"\\n\\\n",
    "    cellsize      \"+str(int(src.res[0]))+\"\\n\\\n",
    "    NODATA_value  -9999\\n\"\n",
    "    np.savetxt(OUTpath+'DEM_'+domain+'.asc', dem, fmt='%4.0f', header = head,comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Met files \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emilio cfsv2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "INpath = 'CFSv2_OperationalAnalysis_WY_40days.nc'\n",
    "OUTpath = 'outmet.dat'#GEEpath+outfilename\n",
    "def MET2SM(INpath, OUTpath):\n",
    "    #compute number of grid points and time steps from size of 3d matrix\n",
    "    data = xr.open_dataset('CFSv2_OperationalAnalysis_WY_40days.nc')\n",
    "    \n",
    "    Z = data['Geopotential_height_surface']\n",
    "\n",
    "    PR = data['Precipitation_rate_surface_6_Hour_Average']\n",
    "\n",
    "    H = data['Specific_humidity_height_above_ground']\n",
    "\n",
    "    P = data['Pressure_surface']\n",
    "\n",
    "    T = data['Temperature_height_above_ground']\n",
    "\n",
    "    U = data['u-component_of_wind_height_above_ground']\n",
    "\n",
    "    V = data['v-component_of_wind_height_above_ground']\n",
    "\n",
    "    #compute number of grid points and time steps from size of 3d matrix\n",
    "    t,y,x=PR.shape\n",
    "    gridpts=x*y\n",
    "    tsteps=t\n",
    "\n",
    "    #create y m d h vectors\n",
    "    year = pd.to_datetime(data.time.values).year\n",
    "    month = pd.to_datetime(data.time.values).month\n",
    "    day = pd.to_datetime(data.time.values).day\n",
    "    hour = pd.to_datetime(data.time.values).hour\n",
    "    \n",
    "    #create ID numbers for the grid points\n",
    "    ID=1000000+np.linspace(1,gridpts,gridpts)\n",
    "\n",
    "    #create matrices of x and y values\n",
    "    X, Y = np.meshgrid(data.easting.values, data.northing.values)\n",
    "    X=X.flatten()\n",
    "    Y=Y.flatten()\n",
    "\n",
    "    #elevation is static (doesn't change with time)\n",
    "    elev=Z[1,:,:].values.flatten()\n",
    "\n",
    "    #find number of grid points with <0 elevation. Note: this is related to the\n",
    "    #subroutine met_data_check in the preprocess_code.f. that subroutine seems\n",
    "    #to suggest that negative elevations are ok (say, death valley). But, the\n",
    "    #code itself checks for negative elevations and stops execution is any\n",
    "    #negatives are found.\n",
    "    I = np.where(elev>=0)\n",
    "    validgridpts=np.shape(I)[1]\n",
    "\n",
    "    #remove data at points with neg elevations\n",
    "    ID=ID[I]\n",
    "    X=X[I]\n",
    "    Y=Y[I]\n",
    "    elev=elev[I]\n",
    "\n",
    "    #we are now ready to begin our main loop over the time steps.\n",
    "    fid= open(OUTpath,\"w+\")\n",
    "\n",
    "    for j in range(tsteps):\n",
    "        #first we write the number of grid points\n",
    "        fid.write('{0:6d}\\n'.format(validgridpts))\n",
    "\n",
    "        #prep data matrix for this time step. First, grab the jth time slice\n",
    "        Prtmp=PR[j,:,:].values.flatten()\n",
    "        Htmp=H[j,:,:].values.flatten()\n",
    "        Ptmp=P[j,:,:].values.flatten()\n",
    "        Ttmp=T[j,:,:].values.flatten()\n",
    "        Utmp=U[j,:,:].values.flatten()\n",
    "        Vtmp=V[j,:,:].values.flatten()\n",
    "\n",
    "        #remove data at points with neg elevations\n",
    "        Prtmp=Prtmp[I]\n",
    "        Htmp=Htmp[I]\n",
    "        Ptmp=Ptmp[I]\n",
    "        Ttmp=Ttmp[I]\n",
    "        Utmp=Utmp[I]\n",
    "        Vtmp=Vtmp[I]\n",
    "\n",
    "\n",
    "        #convert precip rate to precip DEPTH (mm) during time interval\n",
    "        Prtmp=Prtmp*24*3600/pointsperday\n",
    "\n",
    "        #convert specific hum. to RH from Clausius-Clapeyron. T is still in K\n",
    "        RHtmp=0.263*Ptmp*Htmp*(np.exp(17.67*(Ttmp-273.16)/(Ttmp-29.65)))**(-1)\n",
    "\n",
    "        #compute wind speed\n",
    "        SPDtmp=np.sqrt(Utmp**2+Vtmp**2)\n",
    "\n",
    "        #compute wind direction. 0-360, with 0 being true north! 90 east, etc.\n",
    "        DIRtmp=np.arctan2(Utmp,Vtmp)\n",
    "        K=np.where(DIRtmp>=180)\n",
    "        J=np.where(DIRtmp<180)\n",
    "        DIRtmp[K]=DIRtmp[K]+180\n",
    "        DIRtmp[J]=DIRtmp[J]+180\n",
    "\n",
    "        #put T in C\n",
    "        Ttmp=Ttmp-273.16\n",
    "\n",
    "        for z in range(len(Prtmp)): \n",
    "\n",
    "            fid.write('{0:5d}'.format(year[j])+'{0:3d}'.format(month[j])+\n",
    "                      '{0:3d}'.format(day[j])+'{:6.3f}'.format(hour[j])+\n",
    "                      '{0:9d}'.format(int(ID[z]))+'{:12.1f}'.format(X[z])+\n",
    "                      '{:12.1f}'.format(Y[z])+'{:8.1f}'.format(elev[z])+\n",
    "                      '{:9.2f}'.format(Ttmp[z])+'{:9.2f}'.format(RHtmp[z])+\n",
    "                      '{:9.2f}'.format(SPDtmp[z])+'{:9.2f}'.format(DIRtmp[z])+\n",
    "                      '{:9.2f}\\n'.format(Prtmp[z]))\n",
    "    fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MET2SM(INpath, OUTpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEE cfsv2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INpath = GEEpath+metfilename\n",
    "OUTpath = GEEpath+outfilename\n",
    "def MET2SM(INpath, OUTpath):\n",
    "    #compute number of grid points and time steps from size of 3d matrix\n",
    "\n",
    "    Z = xr.open_rasterio(INpath+'_elev.tif')\n",
    "\n",
    "    PR = xr.open_rasterio(INpath+'_prec.tif')\n",
    "\n",
    "    H = xr.open_rasterio(INpath+'_spechum.tif')\n",
    "\n",
    "    P = xr.open_rasterio(INpath+'_surfpres.tif')\n",
    "\n",
    "    T = xr.open_rasterio(INpath+'_tair.tif')\n",
    "\n",
    "    U = xr.open_rasterio(INpath+'_uwind.tif')\n",
    "\n",
    "    V = xr.open_rasterio(INpath+'_vwind.tif')\n",
    "\n",
    "    #compute number of grid points and time steps from size of 3d matrix\n",
    "    t,y,x=PR.shape\n",
    "    gridpts=x*y\n",
    "    tsteps=t\n",
    "\n",
    "    #create y m d h vectors\n",
    "    start_date =datetime.strptime(str(startyear)+'-'+str(startmonth)+'-'\\\n",
    "                                  +str(startday)+'-'+str(starthour),'%Y-%m-%d-%H')\n",
    "    n = str(24/pointsperday)\n",
    "    timestamp = pd.date_range(start_date, periods=tsteps, freq=n+'H')\n",
    "\n",
    "    #create ID numbers for the grid points\n",
    "    ID=1000000+np.linspace(1,gridpts,gridpts)\n",
    "\n",
    "    #create matrices of x and y values\n",
    "    transform = Affine.from_gdal(*PR.attrs['transform'])\n",
    "    X, Y = np.meshgrid(np.arange(x)+0.5, np.arange(y)+0.5) * transform\n",
    "    X=X.flatten()\n",
    "    Y=Y.flatten()\n",
    "\n",
    "    #elevation is static (doesn't change with time)\n",
    "    elev=Z[1,:,:].values.flatten()\n",
    "\n",
    "    #find number of grid points with <0 elevation. Note: this is related to the\n",
    "    #subroutine met_data_check in the preprocess_code.f. that subroutine seems\n",
    "    #to suggest that negative elevations are ok (say, death valley). But, the\n",
    "    #code itself checks for negative elevations and stops execution is any\n",
    "    #negatives are found.\n",
    "    I = np.where(elev>=0)\n",
    "    validgridpts=np.shape(I)[1]\n",
    "\n",
    "    #remove data at points with neg elevations\n",
    "    ID=ID[I]\n",
    "    X=X[I]\n",
    "    Y=Y[I]\n",
    "    elev=elev[I]\n",
    "\n",
    "    #we are now ready to begin our main loop over the time steps.\n",
    "    fid= open(OUTpath,\"w+\")\n",
    "\n",
    "    for j in range(tsteps):\n",
    "        #first we write the number of grid points\n",
    "        fid.write('{0:6d}\\n'.format(validgridpts))\n",
    "\n",
    "        #prep data matrix for this time step. First, grab the jth time slice\n",
    "        Prtmp=PR[j,:,:].values.flatten()\n",
    "        Htmp=H[j,:,:].values.flatten()\n",
    "        Ptmp=P[j,:,:].values.flatten()\n",
    "        Ttmp=T[j,:,:].values.flatten()\n",
    "        Utmp=U[j,:,:].values.flatten()\n",
    "        Vtmp=V[j,:,:].values.flatten()\n",
    "\n",
    "        #remove data at points with neg elevations\n",
    "        Prtmp=Prtmp[I]\n",
    "        Htmp=Htmp[I]\n",
    "        Ptmp=Ptmp[I]\n",
    "        Ttmp=Ttmp[I]\n",
    "        Utmp=Utmp[I]\n",
    "        Vtmp=Vtmp[I]\n",
    "\n",
    "\n",
    "        #convert precip rate to precip DEPTH (mm) during time interval\n",
    "        Prtmp=Prtmp*24*3600/pointsperday\n",
    "\n",
    "        #convert specific hum. to RH from Clausius-Clapeyron. T is still in K\n",
    "        RHtmp=0.263*Ptmp*Htmp*(np.exp(17.67*(Ttmp-273.16)/(Ttmp-29.65)))**(-1)\n",
    "\n",
    "        #compute wind speed\n",
    "        SPDtmp=np.sqrt(Utmp**2+Vtmp**2)\n",
    "\n",
    "        #compute wind direction. 0-360, with 0 being true north! 90 east, etc.\n",
    "        DIRtmp=np.arctan2(Utmp,Vtmp)\n",
    "        K=np.where(DIRtmp>=180)\n",
    "        J=np.where(DIRtmp<180)\n",
    "        DIRtmp[K]=DIRtmp[K]+180\n",
    "        DIRtmp[J]=DIRtmp[J]+180\n",
    "\n",
    "        #put T in C\n",
    "        Ttmp=Ttmp-273.16\n",
    "\n",
    "        for z in range(len(Prtmp)):\n",
    "\n",
    "            fid.write('{0:5d}'.format(timestamp[j].year)+'{0:3d}'.format(timestamp[j].month)+\n",
    "                      '{0:3d}'.format(timestamp[j].day)+'{:6.3f}'.format(timestamp[j].hour)+\n",
    "                      '{0:9d}'.format(int(ID[z]))+'{:12.1f}'.format(X[z])+\n",
    "                      '{:12.1f}'.format(Y[z])+'{:8.1f}'.format(elev[z])+\n",
    "                      '{:9.2f}'.format(Ttmp[z])+'{:9.2f}'.format(RHtmp[z])+\n",
    "                      '{:9.2f}'.format(SPDtmp[z])+'{:9.2f}'.format(DIRtmp[z])+\n",
    "                      '{:9.2f}\\n'.format(Prtmp[z]))\n",
    "    fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/attic/dfh/miniconda/envs/snowmodelcal/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Right multiplication will be prohibited in version 3.0\n"
     ]
    }
   ],
   "source": [
    "MET2SM(INpath, 'outmet.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extramet \n",
    "Lat lon grids for large domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% In this section we write out the optional files containing lat / lon values.\n",
    "% Either grads or ascii is acceptable. I prefer ascii...\n",
    "\n",
    "if FLAG_DEM_EXTRA\n",
    "   disp('Converting projected coords to lat / lon')\n",
    "   disp(' ')\n",
    "    %load file.\n",
    "    [DEM,R_dem]=geotiffread(fullfile(pathname,demname));\n",
    "    %get projection info\n",
    "    proj=geotiffinfo(fullfile(pathname,demname));\n",
    "    %convert projection info to mapping structure\n",
    "    mstruct=geotiff2mstruct(proj);\n",
    "\n",
    "    %create matrices and vectors of x and y values\n",
    "    info=geotiffinfo(fullfile(pathname,demname));\n",
    "    [X,Y]=pixcenters(info,'makegrid');\n",
    "    x=X(1,:);\n",
    "    y=Y(:,1);\n",
    "\n",
    "    %convert coords from projected to geographic\n",
    "    [LAT,LON]=minvtran(mstruct,X,Y);\n",
    "\n",
    "    %write out files\n",
    "    arcgridwrite(fullfile(pathname,'grid_lat.asc'),x,y,LAT,'grid_mapping','center','precision',5);\n",
    "    arcgridwrite(fullfile(pathname,'_grid_lon.asc'),x,y,LON,'grid_mapping','center','precision',5);\n",
    "%     arcgridwrite(fullfile(pathname,[domain 'grid_lat.asc']),x,y,LAT,'grid_mapping','center','precision',5);\n",
    "%     arcgridwrite(fullfile(pathname,[domain '_grid_lon.asc']),x,y,LON,'grid_mapping','center','precision',5);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR2SM():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #compute wind direction. 0-360, with 0 being true north! 90 east, etc.\n",
    "        DIRtmp=np.arctan2(Vtmp,Utmp)* 180 / np.pi\n",
    "        J=np.where((DIRtmp>=0) & (DIRtmp<=90))\n",
    "        K=np.where(DIRtmp>90)\n",
    "        L=np.where((DIRtmp<0) & (DIRtmp>=-90))\n",
    "        M=np.where(DIRtmp<-90)\n",
    "        DIRtmp[J]=90-DIRtmp[J]+180\n",
    "        DIRtmp[K]=270 - DIRtmp[K]\n",
    "        DIRtmp[L]=np.abs(90+ DIRtmp[L])\n",
    "        DIRtmp[M]=270+np.abs(DIRtmp[M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowmodelcal]",
   "language": "python",
   "name": "conda-env-snowmodelcal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
